{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095816da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cacc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SECONDARY_SPLIT = False   \n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "# HEADERS_TO_SPLIT = [\n",
    "#     (\"## Abstract\", \"H1\"),\n",
    "#     (\"## Introduction\", \"H1\"),\n",
    "#     (\"## Results\", \"H1\"),\n",
    "#     (\"## Discussion\", \"H1\"),\n",
    "#     (\"## Results and Discussion\", \"H1\"),\n",
    "#     (\"## Conclusion\", \"H1\"),\n",
    "#     (\"## Conclusions\", \"H1\"),\n",
    "# ]\n",
    "HEADERS_TO_SPLIT = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]                              # Number of results\n",
    "\n",
    "SECTION_COLLECTION_MAP = {\n",
    "    \"Abstract\": \"abstracts\",\n",
    "    \"Introduction\": \"introductions\",\n",
    "    \"Results\": \"results_discussion\",\n",
    "    \"Result\": \"results_discussion\",\n",
    "    \"Discussion\": \"results_discussion\",\n",
    "    \"Results and Discussion\": \"results_discussion\",\n",
    "    \"Conclusion\": \"conclusions\",\n",
    "    \"Conclusions\": \"conclusions\",\n",
    "    \"Background\": \"introductions\",   # Asignamos background a introductions\n",
    "    \"Summary\": \"conclusions\",        # Asignamos summary a conclusions\n",
    "}                            # Number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d7cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_header(header: str) -> str:\n",
    "    \"\"\"Normaliza un header: minúsculas y elimina numeración inicial tipo '1. Introduction'.\"\"\"\n",
    "    header = header.strip().lower()\n",
    "    header = re.sub(r'^\\d+\\.\\s*', '', header)\n",
    "    return header\n",
    "\n",
    "def extract_metadata_from_md(text: str):\n",
    "    \"\"\"Extrae metadatos clave desde un Markdown.\"\"\"\n",
    "    metadata = {}\n",
    "\n",
    "    titles_n1 = re.findall(r'^# (.+)', text, flags=re.MULTILINE)\n",
    "    if titles_n1:\n",
    "        metadata[\"title\"] = titles_n1[0]\n",
    "\n",
    "    pre_authors_section = text.split(\"* Author information\")[0]\n",
    "    authors = re.findall(r'^###\\s+(.+)', pre_authors_section, flags=re.MULTILINE)\n",
    "    if authors:\n",
    "        metadata[\"authors\"] = \", \".join(authors)\n",
    "\n",
    "    author_info_match = re.search(r'### .+?\\n([\\s\\S]*?)(?=\\nFind articles)', text)\n",
    "    if author_info_match:\n",
    "        metadata[\"author_information\"] = author_info_match.group(1).strip()\n",
    "\n",
    "    notes_match = re.search(r'(Received .*?Issue date.*?\\.)', text)\n",
    "    if notes_match:\n",
    "        metadata[\"article_notes\"] = notes_match.group(1).strip()\n",
    "\n",
    "    copyright_match = re.search(r'(Copyright ©.*?Microbiology)', text)\n",
    "    if copyright_match:\n",
    "        metadata[\"copyright_license\"] = copyright_match.group(1).strip()\n",
    "\n",
    "    pmcid_match = re.search(r'PMCID:\\s*(PMC\\d+)', text)\n",
    "    if pmcid_match:\n",
    "        metadata[\"pmcid\"] = pmcid_match.group(1).strip()\n",
    "\n",
    "    pmid_match = re.search(r'PMID:\\s*\\[(\\d+)\\]', text)\n",
    "    if pmid_match:\n",
    "        metadata[\"pmid\"] = pmid_match.group(1).strip()\n",
    "\n",
    "    # Extraer fecha (simple)\n",
    "    date_match = re.search(r'\\b(19|20)\\d{2}\\s+[A-Za-z]{3}(?:\\s*\\d{1,2})?', text)\n",
    "    if date_match:\n",
    "        metadata[\"publication_date\"] = date_match.group(0)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def process_markdown_directory_2(in_dir: str):\n",
    "    \"\"\"Procesa Markdown y guarda en Qdrant por secciones.\"\"\"\n",
    "\n",
    "    in_dir = Path(in_dir)\n",
    "    all_docs = []\n",
    "    counts = defaultdict(int)\n",
    "    all_metadata = {}\n",
    "\n",
    "    # --- 1) Leer archivos ---\n",
    "    for md_path in in_dir.rglob(\"*.md\"):\n",
    "        text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        docs = split_markdown_by_section_dynamic(text)\n",
    "        metadata = extract_metadata_from_md(text)\n",
    "        pmcid = metadata.get(\"pmcid\", md_path.stem)\n",
    "        all_metadata[pmcid] = metadata\n",
    "\n",
    "        for d in docs:\n",
    "            d.metadata.update(metadata)\n",
    "\n",
    "        all_docs.extend(docs)\n",
    "        counts[md_path.as_posix()] += len(docs)\n",
    "\n",
    "    # Guardar metadata completa en JSON\n",
    "    with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Collected {len(all_docs)} chunks from {len(counts)} files.\")\n",
    "\n",
    "    # --- 2) Embeddings ---\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "\n",
    "    # --- 3) Qdrant client ---\n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "    # --- 4) Crear colecciones por sección ---\n",
    "    section_names = set(d.metadata[\"section_collection\"] for d in all_docs)\n",
    "    for col_name in section_names:\n",
    "        try:\n",
    "            client.get_collection(collection_name=col_name)\n",
    "        except Exception:\n",
    "            client.create_collection(\n",
    "                collection_name=col_name,\n",
    "                vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "            )\n",
    "\n",
    "    # --- 5) Separar por colección ---\n",
    "    sections_docs = defaultdict(list)\n",
    "    for d in all_docs:\n",
    "        col_name = d.metadata[\"section_collection\"]\n",
    "        sections_docs[col_name].append(d)\n",
    "\n",
    "    # --- VALIDACIÓN: imprimir 2 primeros chunks por colección ---\n",
    "    for col_name, docs in sections_docs.items():\n",
    "        print(f\"\\n=== Collection: {col_name} | {len(docs)} chunks ===\")\n",
    "        for d in docs[:10]:  # solo los 2 primeros\n",
    "            # print(f\"Title: {d.metadata.get('title')}\")\n",
    "            # print(f\"PMCID: {d.metadata.get('pmcid')}\")\n",
    "            print(f\"section_collection: {d.metadata.get('section_collection')}\")\n",
    "            print(f\"H1: {d.metadata.get('H1')}\")\n",
    "            print(f\"H2: {d.metadata.get('H2')}\")\n",
    "            print(f\"H3: {d.metadata.get('H3')}\")\n",
    "            print(f\"Header: {d.metadata.get('Header')}\")\n",
    "            # print(f\"Content snippet: {d.page_content[:30]}...\\n\")\n",
    "    BATCH_SIZE = 50\n",
    "    SAFE_COLLECTIONS = [\"abstracts\", \"introductions\", \"conclusions\"]\n",
    "    PROBLEM_COLLECTIONS = [\"results_discussion\"]\n",
    "\n",
    "    # --- 5a) Subir colecciones seguras ---\n",
    "    for col_name in SAFE_COLLECTIONS:\n",
    "        docs = sections_docs.get(col_name, [])\n",
    "        if not docs:\n",
    "            continue\n",
    "        store = QdrantVectorStore(client=client, collection_name=col_name, embedding=embeddings)\n",
    "        total = len(docs)\n",
    "        print(f\"Uploading {total} chunks to collection '{col_name}' in batches of {BATCH_SIZE}...\")\n",
    "        for i in range(0, total, BATCH_SIZE):\n",
    "            batch = docs[i:i+BATCH_SIZE]\n",
    "            store.add_documents(batch)\n",
    "            print(f\"  Uploaded batch {i//BATCH_SIZE + 1} ({len(batch)} chunks)\")\n",
    "        print(f\"Collection '{col_name}' uploaded successfully!\")\n",
    "\n",
    "    # # --- 5b) Guardar chunks problemáticos para reintento ---\n",
    "    # for col_name in PROBLEM_COLLECTIONS:\n",
    "    #     docs = sections_docs.get(col_name, [])\n",
    "    #     if docs:\n",
    "    #         with open(f\"problem_chunks_{col_name}.pkl\", \"wb\") as f:\n",
    "    #             pickle.dump(docs, f)\n",
    "    #         print(f\"{len(docs)} chunks from '{col_name}' saved for reattempt later\")\n",
    "\n",
    "    # --- 5b) Guardar chunks problemáticos para reintento ---\n",
    "    for col_name in PROBLEM_COLLECTIONS:\n",
    "        docs = sections_docs.get(col_name, [])\n",
    "        if docs:\n",
    "            failed_chunks = []\n",
    "            embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "            \n",
    "            for i, d in enumerate(docs):\n",
    "                try:\n",
    "                    # Probar embedding solo para este chunk\n",
    "                    emb = embeddings.embed_documents([d.page_content])\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Chunk {i} in '{col_name}' failed: {e}\")\n",
    "                    failed_chunks.append(d)\n",
    "            \n",
    "            # Guardar solo los chunks que fallaron\n",
    "            if failed_chunks:\n",
    "                with open(f\"problem_chunks_{col_name}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(failed_chunks, f)\n",
    "                print(f\"{len(failed_chunks)} chunks from '{col_name}' saved for reattempt later\")\n",
    "\n",
    "    # --- Stats finales ---\n",
    "    print(\"\\nChunks per file (desc):\")\n",
    "    for src, c in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{c:5d}  {src}\")\n",
    "    total = sum(counts.values())\n",
    "    avg = total / len(counts) if counts else 0\n",
    "    print(f\"\\nFiles: {len(counts)} | Total chunks: {total} | Avg/file: {avg:.2f}\")\n",
    "    \n",
    "def has_meaningful_content_for_collection(text: str, collection: str) -> bool:\n",
    "    \"\"\"\n",
    "    Retorna True si el chunk tiene contenido suficiente para la colección indicada.\n",
    "    \"\"\"\n",
    "    text_clean = text.strip()\n",
    "    if not text_clean:\n",
    "        return False\n",
    "\n",
    "    # Por ejemplo, ignorar chunks con menos de 5 palabras\n",
    "    if len(text_clean.split()) < 5:\n",
    "        return False\n",
    "\n",
    "    # Validaciones específicas por colección\n",
    "    first_line = text_clean.split(\"\\n\")[0].lower()\n",
    "    if collection == \"abstracts\":\n",
    "        forbidden = [\"background\", \"introduction\", \"methods\", \"study design\", \"results\", \"discussion\"]\n",
    "    elif collection == \"results_discussion\":\n",
    "        forbidden = [\"abstract\", \"background\", \"introduction\", \"conclusion\", \"summary\"]\n",
    "    elif collection == \"conclusions\":\n",
    "        forbidden = [\"abstract\", \"background\", \"introduction\", \"results\", \"discussion\"]\n",
    "    elif collection == \"introductions\":\n",
    "        forbidden = [\"abstract\", \"results\", \"discussion\", \"conclusion\", \"summary\"]\n",
    "    else:\n",
    "        forbidden = []\n",
    "\n",
    "    return not any(first_line.startswith(f) for f in forbidden)\n",
    "\n",
    "def split_markdown_by_section_dynamic(text: str):\n",
    "    \"\"\"\n",
    "    Divide Markdown en chunks por sección y asigna la colección correspondiente.\n",
    "    Evita guardar chunks vacíos o con contenido que no coincide con el header.\n",
    "    \"\"\"\n",
    "\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=HEADERS_TO_SPLIT,\n",
    "        strip_headers=True,  # elimina los headers del contenido\n",
    "    )\n",
    "    \n",
    "    docs = splitter.split_text(text)\n",
    "    SECTION_COLLECTION_MAP_LOWER = {k.lower(): v for k, v in SECTION_COLLECTION_MAP.items()}\n",
    "\n",
    "    def has_meaningful_content(text: str) -> bool:\n",
    "        \"\"\"Verifica si un chunk tiene contenido real\"\"\"\n",
    "        text_clean = text.strip()\n",
    "        if not text_clean:\n",
    "            return False\n",
    "        # Opcional: ignorar solo headers o líneas muy cortas\n",
    "        if len(text_clean.split()) < 5:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def looks_like_abstract(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica si el texto parece un Abstract real\n",
    "        Evita Background, Introduction, Methods, Results, Discussion al inicio\n",
    "        \"\"\"\n",
    "        first_line = text.strip().split(\"\\n\")[0].lower() if text else \"\"\n",
    "        forbidden = [\"background\", \"introduction\", \"methods\", \"study design\", \"results\", \"discussion\"]\n",
    "        return not any(first_line.startswith(f) for f in forbidden)\n",
    "\n",
    "    section_docs = []\n",
    "\n",
    "    for d in docs:\n",
    "        # Saltar chunks sin contenido\n",
    "        if not has_meaningful_content(d.page_content):\n",
    "            continue\n",
    "\n",
    "        # Tomar headers de mayor a menor: H3 > H2 > H1\n",
    "        headers = [(h, d.metadata.get(h, \"\")) for h in [\"H3\", \"H2\", \"H1\"] if d.metadata.get(h)]\n",
    "        collection = None\n",
    "\n",
    "        for h_name, h_value in headers:\n",
    "            h_norm = normalize_header(h_value)\n",
    "            if h_norm in SECTION_COLLECTION_MAP_LOWER:\n",
    "                candidate_collection = SECTION_COLLECTION_MAP_LOWER[h_norm]\n",
    "\n",
    "                if not has_meaningful_content_for_collection(d.page_content, candidate_collection):\n",
    "                  continue  # Ignorar chunks que no tienen contenido real\n",
    "\n",
    "                collection = candidate_collection\n",
    "                break\n",
    "\n",
    "        if not collection:\n",
    "            continue  # ignorar si ningún header es válido\n",
    "\n",
    "        # Guardar metadata\n",
    "        d.metadata[\"Header\"] = \" | \".join([h_value for _, h_value in headers])\n",
    "        d.metadata[\"H1\"] = d.metadata.get(\"H1\", \"\")\n",
    "        d.metadata[\"H2\"] = d.metadata.get(\"H2\", \"\")\n",
    "        d.metadata[\"H3\"] = d.metadata.get(\"H3\", \"\")\n",
    "        d.metadata[\"section_collection\"] = collection\n",
    "\n",
    "        section_docs.append(d)\n",
    "\n",
    "    return section_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_markdown_directory_2(\n",
    "    in_dir=\"md_out\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31bdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results_discussion_only(in_dir: str):\n",
    "    \"\"\"\n",
    "    Procesa Markdown, extrae chunks de 'results_discussion' y los sube a Qdrant uno por uno.\n",
    "    Guarda los chunks fallidos en un archivo .pkl.\n",
    "    \"\"\"\n",
    "\n",
    "    in_dir = Path(in_dir)\n",
    "    all_docs = []\n",
    "    all_metadata = {}\n",
    "\n",
    "    # --- 1) Leer archivos ---\n",
    "    for md_path in in_dir.rglob(\"*.md\"):\n",
    "        text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        docs = split_markdown_by_section_dynamic(text)  # tu función dinámica de secciones\n",
    "        metadata = extract_metadata_from_md(text)\n",
    "        pmcid = metadata.get(\"pmcid\", md_path.stem)\n",
    "        all_metadata[pmcid] = metadata\n",
    "\n",
    "        for d in docs:\n",
    "            d.metadata.update(metadata)\n",
    "\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    # Guardar metadata completa en JSON\n",
    "    with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_metadata, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Collected {len(all_docs)} chunks from {len(list(in_dir.rglob('*.md')))} files.\")\n",
    "\n",
    "    # --- 2) Embeddings ---\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "\n",
    "    # --- 3) Qdrant client ---\n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "    # --- 4) Crear colección 'results_discussion' si no existe ---\n",
    "    COLLECTION_NAME = \"results_discussion\"\n",
    "    try:\n",
    "        client.get_collection(collection_name=COLLECTION_NAME)\n",
    "    except Exception:\n",
    "        print(f\"Creando colección '{COLLECTION_NAME}'...\")\n",
    "        client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    # --- 5) Filtrar solo chunks de results_discussion ---\n",
    "    docs = [d for d in all_docs if d.metadata.get(\"section_collection\") == COLLECTION_NAME]\n",
    "\n",
    "    if not docs:\n",
    "        print(f\"No hay chunks de '{COLLECTION_NAME}' para subir.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Subiendo {len(docs)} chunks de '{COLLECTION_NAME}' uno por uno...\")\n",
    "\n",
    "    store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME, embedding=embeddings)\n",
    "    failed_chunks = []\n",
    "\n",
    "    for i, d in enumerate(docs):\n",
    "        try:\n",
    "            store.add_documents([d])  # subir uno por uno\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Chunk {i} falló: {e}\")\n",
    "            failed_chunks.append(d)\n",
    "\n",
    "    # --- Guardar chunks fallidos ---\n",
    "    if failed_chunks:\n",
    "        with open(f\"failed_chunks_{COLLECTION_NAME}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(failed_chunks, f)\n",
    "        print(f\"{len(failed_chunks)} chunks fallidos guardados en 'failed_chunks_{COLLECTION_NAME}.pkl'.\")\n",
    "\n",
    "    print(f\"Subida de '{COLLECTION_NAME}' finalizada. Total exitosos: {len(docs) - len(failed_chunks)} | Fallidos: {len(failed_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_results_discussion_only(\"md_out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
