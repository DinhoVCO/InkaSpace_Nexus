{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095816da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61cacc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SECONDARY_SPLIT = False   \n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "HEADERS_TO_SPLIT = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]                               # Number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152100aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown(text: str):\n",
    "    doc = Document(page_content=text)\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            model_name=\"gpt-4\",chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "    docs = splitter.split_documents([doc])\n",
    "    return docs\n",
    "    \n",
    "\n",
    "def process_markdown_directory(in_dir: str, collection: str):\n",
    "    \"\"\"Main function to process markdown files and store them into a local Qdrant vector database.\"\"\"\n",
    "    \n",
    "    in_dir = Path(in_dir)\n",
    "\n",
    "    # 1) Collect & split\n",
    "    all_docs = []\n",
    "    counts = defaultdict(int)\n",
    "    for md_path in in_dir.rglob(\"*.md\"):\n",
    "        text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        docs = split_markdown(text)\n",
    "        for d in docs:\n",
    "            d.metadata = {\"source\": md_path.as_posix(), **(d.metadata or {})}\n",
    "        all_docs.extend(docs)\n",
    "        counts[md_path.as_posix()] += len(docs)\n",
    "\n",
    "    print(f\"Collected {len(all_docs)} chunks from {len(counts)} files.\")\n",
    "\n",
    "    # 2) Embeddings\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "    # 3) Local embedded Qdrant (persists under qdrant_dir)\n",
    "    client = QdrantClient(\n",
    "        url=QDRANT_URL, \n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "\n",
    "    # 4) Create collection if missing (size must match embedding dim)\n",
    "    vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "    try:\n",
    "        client.get_collection(collection_name=collection)\n",
    "    except Exception:\n",
    "        client.create_collection(\n",
    "            collection_name=collection,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    # 5) Upsert via LangChain QdrantVectorStore\n",
    "    store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    store.add_documents(all_docs)\n",
    "\n",
    "    print(f\"Qdrant collection: {collection}\")\n",
    "\n",
    "    # Simple per-file stats\n",
    "    print(\"\\nChunks per file (desc):\")\n",
    "    for src, c in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{c:5d}  {src}\")\n",
    "    total = sum(counts.values())\n",
    "    avg = total / len(counts) if counts else 0\n",
    "    print(f\"\\nFiles: {len(counts)} | Total chunks: {total} | Avg/file: {avg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9755f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#frontend\\RAG\\md_out\\www.ncbi.nlm.nih.gov\\pmc\\articles\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mprocess_markdown_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmd_out\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbiology_articles\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mprocess_markdown_directory\u001b[39m\u001b[34m(in_dir, collection)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m md_path \u001b[38;5;129;01min\u001b[39;00m in_dir.rglob(\u001b[33m\"\u001b[39m\u001b[33m*.md\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     text = md_path.read_text(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     docs = \u001b[43msplit_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[32m     22\u001b[39m         d.metadata = {\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: md_path.as_posix(), **(d.metadata \u001b[38;5;129;01mor\u001b[39;00m {})}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36msplit_markdown\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      2\u001b[39m doc = Document(page_content=text)\n\u001b[32m      3\u001b[39m splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n\u001b[32m      4\u001b[39m         model_name=\u001b[33m\"\u001b[39m\u001b[33mgpt-4\u001b[39m\u001b[33m\"\u001b[39m,chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n\u001b[32m      5\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m docs = \u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\langchain_text_splitters\\base.py:103\u001b[39m, in \u001b[36mTextSplitter.split_documents\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m    101\u001b[39m     texts.append(doc.page_content)\n\u001b[32m    102\u001b[39m     metadatas.append(doc.metadata)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\langchain_text_splitters\\base.py:86\u001b[39m, in \u001b[36mTextSplitter.create_documents\u001b[39m\u001b[34m(self, texts, metadatas)\u001b[39m\n\u001b[32m     84\u001b[39m index = \u001b[32m0\u001b[39m\n\u001b[32m     85\u001b[39m previous_chunk_len = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     87\u001b[39m     metadata = copy.deepcopy(_metadatas[i])\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_start_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\langchain_text_splitters\\character.py:149\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Split the input text into smaller chunks based on predefined separators.\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        List[str]: A list of text chunks obtained after splitting.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\langchain_text_splitters\\character.py:133\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter._split_text\u001b[39m\u001b[34m(self, text, separators)\u001b[39m\n\u001b[32m    131\u001b[39m             final_chunks.append(s)\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m             other_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_separators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m             final_chunks.extend(other_info)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _good_splits:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\langchain_text_splitters\\character.py:123\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter._split_text\u001b[39m\u001b[34m(self, text, separators)\u001b[39m\n\u001b[32m    121\u001b[39m _separator = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._keep_separator \u001b[38;5;28;01melse\u001b[39;00m separator\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_length_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m < \u001b[38;5;28mself\u001b[39m._chunk_size:\n\u001b[32m    124\u001b[39m         _good_splits.append(s)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\langchain_text_splitters\\base.py:205\u001b[39m, in \u001b[36mTextSplitter.from_tiktoken_encoder.<locals>._tiktoken_encoder\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tiktoken_encoder\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[43menc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dinho\\Desktop\\nasa\\nasa_env\\Lib\\site-packages\\tiktoken\\core.py:124\u001b[39m, in \u001b[36mEncoding.encode\u001b[39m\u001b[34m(self, text, allowed_special, disallowed_special)\u001b[39m\n\u001b[32m    121\u001b[39m         raise_disallowed_special_token(match.group())\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_core_bpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# invalid UTF-8 to Rust, it will rightfully complain. Here we do a quick and dirty\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# string, but given that this is input we want to support, maybe that's okay.\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# Also we use errors=\"replace\" to handle weird things like lone surrogates.\u001b[39;00m\n\u001b[32m    132\u001b[39m     text = text.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m\"\u001b[39m).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#frontend\\RAG\\md_out\\www.ncbi.nlm.nih.gov\\pmc\\articles\n",
    "process_markdown_directory(\n",
    "    in_dir=\"md_out\",\n",
    "    collection=\"biology_articles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a440a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
