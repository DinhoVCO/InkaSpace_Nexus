{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095816da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61cacc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SECONDARY_SPLIT = False   \n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "HEADERS_TO_SPLIT = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]                               # Number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152100aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown(text: str):\n",
    "    doc = Document(page_content=text)\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            model_name=\"gpt-4\",chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "    docs = splitter.split_documents([doc])\n",
    "    return docs\n",
    "    \n",
    "\n",
    "def process_markdown_directory(in_dir: str, collection: str):\n",
    "    \"\"\"Main function to process markdown files and store them into a local Qdrant vector database.\"\"\"\n",
    "    \n",
    "    in_dir = Path(in_dir)\n",
    "\n",
    "    # 1) Collect & split\n",
    "    all_docs = []\n",
    "    counts = defaultdict(int)\n",
    "    for md_path in in_dir.rglob(\"*.md\"):\n",
    "        text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        docs = split_markdown(text)\n",
    "        for d in docs:\n",
    "            d.metadata = {\"source\": md_path.as_posix(), **(d.metadata or {})}\n",
    "        all_docs.extend(docs)\n",
    "        counts[md_path.as_posix()] += len(docs)\n",
    "\n",
    "    print(f\"Collected {len(all_docs)} chunks from {len(counts)} files.\")\n",
    "\n",
    "    # 2) Embeddings\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "    # 3) Local embedded Qdrant (persists under qdrant_dir)\n",
    "    client = QdrantClient(\n",
    "        url=QDRANT_URL, \n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "\n",
    "    # 4) Create collection if missing (size must match embedding dim)\n",
    "    vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "    try:\n",
    "        client.get_collection(collection_name=collection)\n",
    "    except Exception:\n",
    "        client.create_collection(\n",
    "            collection_name=collection,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    # 5) Upsert via LangChain QdrantVectorStore\n",
    "    store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection,\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    store.add_documents(all_docs)\n",
    "\n",
    "    print(f\"Qdrant collection: {collection}\")\n",
    "\n",
    "    # Simple per-file stats\n",
    "    print(\"\\nChunks per file (desc):\")\n",
    "    for src, c in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{c:5d}  {src}\")\n",
    "    total = sum(counts.values())\n",
    "    avg = total / len(counts) if counts else 0\n",
    "    print(f\"\\nFiles: {len(counts)} | Total chunks: {total} | Avg/file: {avg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9755f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frontend\\RAG\\md_out\\www.ncbi.nlm.nih.gov\\pmc\\articles\n",
    "process_markdown_directory(\n",
    "    in_dir=\"md_out\",\n",
    "    collection=\"biology_articles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a440a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
